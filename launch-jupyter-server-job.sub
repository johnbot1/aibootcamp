#!/bin/bash
#
# This slurm submission script will launch a jupyter notebook
# server on one of the reserved gpu nodes for aibootcamp. You
# will need to assure a unique port is being used as multiple 
# jobs will be running on each gpu node.

# Submit this script with the command: sbatch thisfilename.sub
# Slurm directives start with #SBATCH (this is not considered a comment)

# Walltime, set for entire day if needed to avoid having Juputer terminated.
#SBATCH --time=08:00:00   # Hours:Minutes:Seconds

# Nodes required. These are single node jobs so leave at 1
#SBATCH --nodes=1   # number of nodes

# CPU cores required. Each P100 GPU node has 28 cores and 4 P100 GPU's.
# Setting to 7 cores/1 GPU allows maximum use of the node.
#SBATCH --ntasks=7   # number of processor cores (i.e. tasks)

# Memory per-core. Default is 4GB for each core requested. 
# The ## instead of # here is a comment (slurm does not see this directive)
##SBATCH --mem-per-cpu=12G   # memory per CPU core

# 1 GPU will be available for each Jupyter session
#SBATCH --gres=gpu:1

# Use the aibootcamp reservation
#SBATCH --reservation=aibootcamp

# Job Name 
#SBATCH -J "jupyter-server-aibootcamp"

# Save output to a log directory within the aibootcamp directory
# cd to aibootcamp dir before running sbatch

#SBATCH -o ./logs/slurm.%N.%j.out # STDOUT
#SBATCH -e ./logs/slurm.%N.%j.err # STDERR

# Setup individual ports or each jupyter process. Default Jupyter
# port is 8888. Creating a mapping of student to port beforehand
# will avoid collisions. (i.e. 8900,8901,8902 etc)
port=8900

echo "Your Jupyter process is running on node $SLURM_NODELIST on Port $port" 

# Edit this to match the virtual python environment being used. This
# example is for conda. 

source /central/groups/imss_admin/jflilley/miniconda3/etc/profile.d/conda.sh
conda activate jupyter-reza
which python
nohup jupyter-notebook --no-browser --port=$port

